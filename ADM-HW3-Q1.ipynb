{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT LIBRARIES\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import lxml\n",
    "from lxml import html\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1- Reach main URL\n",
    "base_url = 'https://www.findamasters.com/masters-degrees/msc-degrees/?PG=1'\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(base_url)\n",
    "driver.maximize_window()\n",
    "\n",
    "#WAIT FOR PUBLICITY\n",
    "try:\n",
    "    WebDriverWait(driver,20).until(EC.element_to_be_clickable((By.XPATH, \"//*[@id='signupModal']/div/div/div/i\"))).click()\n",
    "except:\n",
    "    print(\"No pop up\")\n",
    "time.sleep(10)\n",
    "\n",
    "#MASTER COURSES URL\n",
    "masters_url = []\n",
    "base_url = 'https://www.findamasters.com/'\n",
    "base_url_page = 'https://www.findamasters.com/masters-degrees/msc-degrees/?PG='\n",
    "for n in range(1,401):\n",
    "    \n",
    "    url = base_url_page + str(n)\n",
    "    res = requests.get(url)\n",
    "    time.sleep(random.uniform(5, 10))\n",
    "    soup = BeautifulSoup(res.text, \"lxml\")\n",
    "    \n",
    "    masters = soup.find_all('a', {'class': 'courseLink text-dark', 'href':True})\n",
    "    for master in masters:\n",
    "        masters_url.append(base_url + master['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1- Checking we collected all the required URL's (6000)\n",
    "print(len(masters_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#1.1- Saving the collected URL's in a txt file\n",
    "with open('urls.txt', 'w') as file:\n",
    "    # Write each URL to a new line\n",
    "    for url in masters_url:\n",
    "        file.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2- Saving our txt folder as a list \n",
    "with open('urls.txt', 'r') as file:\n",
    "    # Read the file line by line\n",
    "    urls = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2- Download the HTML of a URL and save it as a file in a folder\n",
    "def download_and_save(url, i):\n",
    "    try:\n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        html = driver.page_source\n",
    "        folder_name = f'folder_{i // 15}'\n",
    "        if not os.path.exists(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "        file_name = f'{i % 15}.html'\n",
    "        with open(os.path.join(folder_name, file_name), 'w', encoding='utf-8') as f:\n",
    "            f.write(html)\n",
    "        driver.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while downloading {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2- Using multiple thread to execute the 'download_and_save' function\n",
    "import concurrent.futures\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "  # Use the executor to map the function to the URLs\n",
    "  executor.map(download_and_save, urls, range(len(urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due to the large number of pages we should download, we decided to use Python's concurrent.futures module which use multiple threads. Using this method we are able to execute the 'download_and_save' function over multiple URL's concurrently and to decrease the running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3- Creating a Data Frame with the required columns\n",
    "df = pd.DataFrame(columns=[\"courseName\", \"universityName\", \"facultyName\", \"isItFullTime\", \"description\", \"startDate\", \"fee\", \"modality\", \"duration\", \"city\", \"country\", \"administration\", \"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3- Collect the data from each HTML file and add it to the Data Frame\n",
    "for i in range(400):\n",
    "    folder_name = f'folder_{i}'\n",
    "    for j in range(15):\n",
    "        file_name = f'{j}.html'\n",
    "        with open(os.path.join(folder_name, file_name), 'r', encoding='utf-8') as f:\n",
    "            contents = f.read()\n",
    "            soup = BeautifulSoup(contents, 'html.parser')\n",
    "        tree = html.fromstring(str(soup))\n",
    "        e_courseName = tree.xpath('//*[@id=\"MainArea\"]/div[2]/div/div/div/div/div/h1')\n",
    "        for element in e_courseName:\n",
    "            courseName = element.text_content()\n",
    "        e_universityName = tree.xpath('//*[@id=\"MainArea\"]/div[2]/div/div/div/div/div/h3/span/a[1]')\n",
    "        for element in e_universityName:\n",
    "            universityName = element.text_content()\n",
    "        e_facultyName = tree.xpath('//*[@id=\"MainArea\"]/div[4]/div/div/div[1]/div/div[3]/a[1]')\n",
    "        for element in e_facultyName:\n",
    "            facultyName = element.text_content()\n",
    "        e_isItFullTime = tree.xpath('//*[@id=\"MainArea\"]/div[4]/div/div/div[1]/div/div[1]/div/span[1]/a')\n",
    "        for element in e_isItFullTime:\n",
    "            isItFullTime = element.text_content()\n",
    "        e_description = tree.xpath('//*[@id=\"Snippet\"]')\n",
    "        for element in e_description:\n",
    "            description = element.text_content()\n",
    "        e_startDate = tree.xpath('//*[@id=\"MainArea\"]/div[4]/div/div/div[1]/div/div[1]/div/span[2]')\n",
    "        for element in e_startDate:\n",
    "            startDate = element.text_content()\n",
    "        e_fees = tree.xpath('//*[@id=\"MainArea\"]/div[4]/div/div/div[1]/div/div[7]/div/p/a') \n",
    "        for element in e_fees:\n",
    "            fee = element.text_content()\n",
    "        e_modality = tree.xpath('//*[@id=\"MainArea\"]/div[4]/div/div/div[1]/div/div[1]/div/span[3]')\n",
    "        for element in e_modality:\n",
    "            modality = element.text_content()\n",
    "        e_duration = tree.xpath('//*[@id=\"MainArea\"]/div[4]/div/div/div[1]/div/div[1]/div/span[4]')\n",
    "        for element in e_duration:\n",
    "            duration = element.text_content()\n",
    "        e_city = tree.xpath('//*[@id=\"MainArea\"]/div[4]/div/div/div[1]/div/div[3]/a[4]')\n",
    "        for element in e_city:\n",
    "            city = element.text_content()\n",
    "        e_country = tree.xpath('//*[@id=\"MainArea\"]/div[4]/div/div/div[1]/div/div[3]/a[5]')\n",
    "        for element in e_country:\n",
    "            country = element.text_content()\n",
    "        e_administraion = tree.xpath('//*[@id=\"MainArea\"]/div[4]/div/div/div[1]/div/div[3]/a[6]')\n",
    "        for element in e_administraion:\n",
    "            administraion = element.text_content()\n",
    "        url = urls[i]\n",
    "        df = df.append({\"courseName\": courseName, \"universityName\": universityName, \"facultyName\": facultyName, \"isItFullTime\": isItFullTime, \"description\": description, \"startDate\": startDate, \"fee\": fee, \"modality\": modality, \"duration\": duration, \"city\": city, \"country\": country, \"administraion\": administraion, \"url\":url}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "#1.3- Checking we added all the data (6000)\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3- Converting the Data Frame to tsv file\n",
    "df.to_csv('courses.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3- Converting the Data Frame to csv file\n",
    "df.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
