{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT LIBRARIES\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import lxml\n",
    "from lxml import html\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Data Collection\n",
    "### 1.1. Get the list of master's degree courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1- Reach main URL\n",
    "base_url = 'https://www.findamasters.com/masters-degrees/msc-degrees/?PG=1'\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(base_url)\n",
    "driver.maximize_window()\n",
    "\n",
    "#WAIT FOR PUBLICITY\n",
    "try:\n",
    "    WebDriverWait(driver,20).until(EC.element_to_be_clickable((By.XPATH, \"//*[@id='signupModal']/div/div/div/i\"))).click()\n",
    "except:\n",
    "    print(\"No pop up\")\n",
    "time.sleep(10)\n",
    "\n",
    "#MASTER COURSES URL\n",
    "masters_url = []\n",
    "base_url = 'https://www.findamasters.com/'\n",
    "base_url_page = 'https://www.findamasters.com/masters-degrees/msc-degrees/?PG='\n",
    "for n in range(1,401):\n",
    "    \n",
    "    url = base_url_page + str(n)\n",
    "    res = requests.get(url)\n",
    "    time.sleep(random.uniform(5, 10))\n",
    "    soup = BeautifulSoup(res.text, \"lxml\")\n",
    "    \n",
    "    masters = soup.find_all('a', {'class': 'courseLink text-dark', 'href':True})\n",
    "    for master in masters:\n",
    "        masters_url.append(base_url + master['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1- Checking we collected all the required URL's (6000)\n",
    "print(len(masters_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#1.1- Saving the collected URL's in a txt file\n",
    "with open('urls.txt', 'w') as file:\n",
    "    # Write each URL to a new line\n",
    "    for url in masters_url:\n",
    "        file.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl master's degree pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2- Saving our txt folder as a list \n",
    "with open('urls.txt', 'r') as file:\n",
    "    # Read the file line by line\n",
    "    urls = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2- Download the HTML of a URL and save it as a file in a folder\n",
    "def download_and_save(url, i):\n",
    "    try:\n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        html = driver.page_source\n",
    "        folder_name = f'folder_{i // 15}'\n",
    "        if not os.path.exists(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "        file_name = f'{i % 15}.html'\n",
    "        with open(os.path.join(folder_name, file_name), 'w', encoding='utf-8') as f:\n",
    "            f.write(html)\n",
    "        driver.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while downloading {url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to the large number of pages we should download, we decided to use Python's concurrent.futures module which use multiple threads. Using this method we are able to execute the 'download_and_save' function over multiple URL's concurrently and to decrease the running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2- Using multiple thread to execute the 'download_and_save' function\n",
    "import concurrent.futures\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "  # Use the executor to map the function to the URLs\n",
    "  executor.map(download_and_save, urls, range(len(urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"courseName\",\n",
    "                \"universityName\",\n",
    "                \"facultyName\",\n",
    "                \"isItFullTime\",\n",
    "                \"description\",\n",
    "                \"startDate\",\n",
    "                \"fees\",\n",
    "                \"modality\",\n",
    "                \"duration\",\n",
    "                \"city\",\n",
    "                \"country\",\n",
    "                \"administration\",\n",
    "                \"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3- Creating a Data Frame with the required columns\n",
    "df = pd.DataFrame(columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.3- Collect the data from each HTML file and add it to the Data Frame\n",
    "for i in range(400):\n",
    "    folder_name = f'folder_{i}'\n",
    "    for j in range(15):\n",
    "        file_name = f'{j}.html'\n",
    "        \n",
    "        with open(os.path.join(folder_name, file_name), 'r', encoding='utf-8') as f:\n",
    "            contents = f.read()\n",
    "            soup = BeautifulSoup(contents, 'html.parser')\n",
    "        \n",
    "        try:\n",
    "            courseName = soup.find(\"h1\", {\"class\": \"course-header__course-title\"}).text\n",
    "        except:\n",
    "            courseName = \"\"\n",
    "        \n",
    "        try:\n",
    "            universityName = soup.find('a', {'class': 'course-header__institution'}).text\n",
    "        except:\n",
    "            universityName = \"\"\n",
    "        \n",
    "        try:\n",
    "            facultyName = soup.find('a', {'class': 'course-header__department'}).text\n",
    "        except:\n",
    "            facultyName = \"\"\n",
    "        \n",
    "        try:\n",
    "            FullTime_links= soup.find_all('a', {'class':'concealLink' })\n",
    "            FullTime = False\n",
    "            for item in FullTime_links:\n",
    "                if item['href']== \"/masters-degrees/full-time/\":\n",
    "                    FullTime = True\n",
    "                    break\n",
    "            isItFullTime= FullTime\n",
    "        except:\n",
    "            isItFullTime= False\n",
    "        \n",
    "        try:\n",
    "            paragraphs = soup.find(\"div\", class_=\"course-sections course-sections__description col-xs-24\")\n",
    "            paragraphs = paragraphs.find(\"div\", id=\"Snippet\").find_all(\"p\")\n",
    "            description = \" \".join([paragraph.text.strip() for paragraph in paragraphs])\n",
    "        except:\n",
    "            description = \"\"\n",
    "        \n",
    "        try:\n",
    "            startDate = soup.find(\"span\", {\"class\": \"key-info__start-date\"}).text\n",
    "        except:\n",
    "            startDate = \"\" \n",
    "        \n",
    "        try:\n",
    "            fee = soup.find(\"div\", {\"class\": \"course-sections__fees\"}).text\n",
    "        except:\n",
    "            fee = \"\"\n",
    "    \n",
    "        try:\n",
    "            modality = soup.find(\"span\", {\"class\": \"key-info__qualification\"}).text\n",
    "        except:\n",
    "            modality = \"\"\n",
    "                \n",
    "        try:\n",
    "            duration = soup.find(\"span\", {\"class\": \"key-info__duration\"}).text\n",
    "        except:\n",
    "            duration = \"\"\n",
    "            \n",
    "        try:\n",
    "            city = soup.find(\"a\", {\"class\": \"card-badge text-wrap text-left badge badge-gray-200 p-2 m-1 font-weight-light course-data course-data__city\"}).text\n",
    "        except:\n",
    "            city= \"\"\n",
    "        \n",
    "        try:\n",
    "            country = soup.find(\"a\", {\"class\": \"card-badge text-wrap text-left badge badge-gray-200 p-2 m-1 font-weight-light course-data course-data__country\"}).text\n",
    "        except:\n",
    "            country = \"\"\n",
    "        \n",
    "        online_course = soup.find(\"a\", {\"class\": \"course-data__online\"})\n",
    "        on_campus_course = soup.find(\"a\", {\"class\": \"course-data__on-campus\"})\n",
    "        if online_course is None and on_campus_course is None:\n",
    "            administration = \"\"\n",
    "        elif online_course is None:\n",
    "            administration = on_campus_course.text\n",
    "        elif on_campus_course is None:\n",
    "            administration = online_course.text\n",
    "        else:\n",
    "            administration = online_course.text + ', ' + on_campus_course.text\n",
    "        \n",
    "        url = soup.find(\"link\", rel=\"canonical\")[\"href\"]\n",
    "    \n",
    "        data = {\"courseName\": courseName, \n",
    "                \"universityName\": universityName, \n",
    "                \"facultyName\": facultyName, \n",
    "                \"isItFullTime\": isItFullTime, \n",
    "                \"description\": description, \n",
    "                \"startDate\": startDate, \n",
    "                \"fees\": fee, \n",
    "                \"modality\": modality, \n",
    "                \"duration\": duration, \n",
    "                \"city\": city, \n",
    "                \"country\": country, \n",
    "                \"administration\": administration, \n",
    "                \"url\": url}\n",
    "        \n",
    "        df_t = pd.DataFrame(columns=column_names)\n",
    "        df_t = pd.concat([df_t, pd.DataFrame([data], columns=df_t.columns)], ignore_index=True)\n",
    "        df_t.to_csv(f\"courses/course_{(i*15) +j}.tsv\", sep=\"\\t\", index=False)\n",
    "        df = df.append(data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "#1.3- Checking we added all the data (6000)\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
